{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef3ebe1-8360-4624-b178-e9b50be03a04",
   "metadata": {},
   "source": [
    "#1\n",
    "\n",
    "Explain the concept of R-squared in linear regression models. How is it calculated, and what does it \n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b7fbd-b0b5-425a-8dd4-36e82ed4a6f4",
   "metadata": {},
   "source": [
    "The R-squared (R²) statistic provides a measure of how well the linear regression model is fitting the actual data. It is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable or variables in a regression model.\n",
    "\n",
    "it is calculated as 1 minus the ratio of SSR to SST, represented as:\n",
    "R^2 = 1 -SSR/SST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da93c19-46d8-4a9a-80ae-edf9997a43ef",
   "metadata": {},
   "source": [
    "#2\n",
    "\n",
    " Define adjusted R-squared and explain how it differs from the regular R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5266bc-73b3-46a2-80ed-9645e82c8a95",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a statistical measure used in regression analysis to evaluate the goodness of fit of a regression model. It is an adjusted or modified version of the regular R-squared (R²) and is designed to provide a more accurate representation of the model's performance, especially when dealing with multiple independent variables.\n",
    "\n",
    "R-squared: It assumes that every single variable explains the variation in the dependent variable. The problem here is that it will always increase as you add more variables to the model, regardless of whether or not those variables are truly meaningful.\n",
    "\n",
    "Adjusted R-squared: It adjusts for the number of terms in the model. It increases only if the new variable improves the model more than would be expected by chance. It incorporates the model’s degrees of freedom, and hence, it can sometimes be a better measure for comparing different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d68d6-db07-4d4c-b74f-2239485929e6",
   "metadata": {},
   "source": [
    "#3\n",
    "\n",
    "When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "---\n",
    "The Adjusted R-squared is more appropriate to use when you are comparing models that have a different number of predictors.\n",
    "\n",
    "In multiple regression analysis, when we add more and more irrelevant variables, the value of R-squared increases but it does not mean that those variables are significant. In this case, even though the added variables are not improving our model, R-squared will still increase.\n",
    "\n",
    "To rectify this problem, we use Adjusted R-squared which penalizes excessive use of such insignificant variables. If you add more and more useless variables to a model, adjusted r-squared will decrease. If you add more useful variables, adjusted r-squared will increase.\n",
    "\n",
    "Adjusted R-squared should always be used with models with more than one predictor variable. It is interpreted as the proportion of total variance that is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ad5f6-e1e7-4bd9-96af-d8adcbe40f45",
   "metadata": {},
   "source": [
    "#4\n",
    "\n",
    "What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics \n",
    "calculated, and what do they represent?\n",
    "\n",
    "---\n",
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are all measures of the differences between values predicted by a model and the values actually observed. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. \n",
    "\n",
    "Here's how these metrics are calculated:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: It is the average of the squared differences between the predicted and actual values. It is calculated as:\n",
    "    $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**: It is the square root of the MSE. Because the MSE is squared, its units do not match that of the original output. RMSE is the square root of MSE, which rectifies this problem by bringing back the error metric to the original units, making interpretation easier. It is calculated as:\n",
    "    $$ RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "3. **Mean Absolute Error (MAE)**: It is the average of the absolute differences between the predicted and actual values. It gives an idea of how wrong the predictions were. It is calculated as:\n",
    "    $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the actual value,\n",
    "- $\\hat{y}_i$ is the predicted value,\n",
    "- $n$ is the total number of data points.\n",
    "\n",
    "These metrics represent:\n",
    "\n",
    "- **MSE**: It represents the average squared difference between the observed actual outturn values and the values predicted by the model. So, it gives a rough idea of the magnitude of error.\n",
    "\n",
    "- **RMSE**: It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. Also, it is a good measure of how accurately the model predicts the response.\n",
    "\n",
    "- **MAE**: It measures the average magnitude of errors in a set of predictions, without considering their direction. It’s averaged absolute difference between predicted and actual values.\n",
    "\n",
    "All these are loss functions because we want to minimize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0313cb-3f2a-4343-815d-0e05d8476c32",
   "metadata": {},
   "source": [
    "#5\n",
    "\n",
    "Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in  regression analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Mean Squared Error (MSE)**:\n",
    "- Advantages:\n",
    "    - It is differentiable, allowing us to use calculus to find minimum or maximum values, which are useful in gradient descent.\n",
    "    - It heavily penalizes larger errors because it squares the residuals.\n",
    "- Disadvantages:\n",
    "    - Since it squares the residuals, it gives more weight to larger errors. This means the model is influenced by outliers, or extreme values.\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**:\n",
    "- Advantages:\n",
    "    - It preserves the unit of the original target variable, making interpretation easier.\n",
    "    - Like MSE, it penalizes larger errors.\n",
    "- Disadvantages:\n",
    "    - Like MSE, it is sensitive to outliers.\n",
    "\n",
    "**Mean Absolute Error (MAE)**:\n",
    "- Advantages:\n",
    "    - It does not square the residuals, so it does not inflate the impact of outliers.\n",
    "    - It preserves the unit of the original target variable.\n",
    "- Disadvantages:\n",
    "    - It is not differentiable at zero, making it less suitable for optimization algorithms that require smoothness for small derivative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829cefa-5a40-472c-a967-2b6b638c2881",
   "metadata": {},
   "source": [
    "#6\n",
    "\n",
    "How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate\n",
    "---\n",
    "---\n",
    "Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the model’s loss function. This penalty term encourages smaller coefficient values, which leads to simpler models that are less likely to overfit.\n",
    "\n",
    "Here’s an example: Let’s say we have a dataset with many features and we’re using a linear regression model. Without regularization, this model might fit the training data very well by assigning large weights to some features. However, these features might be capturing noise in the training data, and when we try to use this model on new data, it performs poorly.\n",
    "\n",
    "Now, if we apply Lasso or Ridge regression (which are forms of regularization), these methods will add a penalty term to the cost function that discourages large weights. As a result, our model will be forced to distribute weights more evenly among features or even set some weights to zero (in case of Lasso). This results in a simpler, more generalized model that performs better on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4554174-8f02-44a2-a4ea-bd926594c034",
   "metadata": {},
   "source": [
    "#7\n",
    "\n",
    "Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "---\n",
    "---\n",
    "Lasso Regression, also known as L1 regularization, is a popular technique used in statistical modeling and machine learning to estimate the relationships between variables and make predictions. The term LASSO stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "The primary goal of LASSO regression is to find a balance between model simplicity and accuracy. It achieves this by adding a penalty term to the traditional linear regression model, which encourages sparse solutions where some coefficients are forced to be exactly zero. This feature makes LASSO particularly useful for feature selection, as it can automatically identify and discard irrelevant or redundant variables.\n",
    "\n",
    "Lasso Regression uses the L1 regularization technique. It is used when we have more features because it automatically performs feature selection.\n",
    "\n",
    "The objective of LASSO regression is to find the values of the coefficients that minimize the sum of the squared differences between the predicted values and the actual values, while also minimizing the L1 regularization term.\n",
    "\n",
    "how does it differ from other regression techniques?\n",
    "\n",
    "Ridge and Lasso’s regression are both powerful techniques for regularizing linear regression models and preventing overfitting. They both add a penalty term to the cost function, but with different approaches. Ridge regression shrinks the coefficients towards zero, while Lasso regression encourages some of them to be exactly zero.\n",
    "\n",
    "The main difference between Ridge and Lasso regression is in the way they shrink the coefficients. Ridge regression can reduce all the coefficients by a small amount but Lasso can reduce some features more than others and hence can completely eliminate those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3eb849-1669-48d6-a39a-e0a2f4f09ff7",
   "metadata": {},
   "source": [
    "#8\n",
    "\n",
    "Discuss the limitations of regularized linear models and explain why they may not always be the best  choice for regression analysis\n",
    "=\n",
    "---\n",
    "\n",
    "1. Data Requirements: Regularized regression models require a larger dataset with a simple model rather than a small dataset with a complex model. Collecting more data is almost always the answer when we want more accurate and more generalizable models.\n",
    "\n",
    "2. Variable Selection: While L1 regularization (used in Lasso) can perform automatic feature selection, it does not replace expert knowledge. The features selected by the model may not always align with domain knowledge or intuition.\n",
    "\n",
    "3. No p-values: Regularized regression does not provide p-values for the regression coefficients. This means it can be harder to interpret the statistical significance of individual predictors.\n",
    "\n",
    "4. Assumption of Linearity: Like ordinary linear regression, regularized linear models also assume a linear relationship between the predictors and the response variable. If this assumption is violated, the model may provide inaccurate predictions.\n",
    "\n",
    "5. Scaling of Data: Regularized regression methods are sensitive to the scale of input features. Therefore, it's important to standardize or normalize the features before applying these methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f389c1f-c8e4-4930-aebd-ffd3b96b5912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
